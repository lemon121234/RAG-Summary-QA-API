"""
RAG æ‘˜è¦èˆ‡QA API
ä½¿ç”¨ FastAPI + Ollama å¯¦ç¾æª¢ç´¢å¢å¼·ç”Ÿæˆï¼ˆRAGï¼‰
æ”¯æ´å¤šæ–‡æª”ä¸Šå‚³ã€å‘é‡æª¢ç´¢ã€æ™ºèƒ½å•ç­”
ï¼ˆè¼•é‡ç‰ˆ - ä¸éœ€è¦é¡å¤–å®‰è£ chromadb å’Œ sentence-transformersï¼‰
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional, Dict
import httpx
import os
import uuid
from datetime import datetime
import json
import math
import re

# ============ é…ç½® ============

OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3.2")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "nomic-embed-text")  # Ollama åµŒå…¥æ¨¡å‹
CHUNK_SIZE = 500  # æ¯å€‹æ–‡æª”ç‰‡æ®µçš„å­—æ•¸
CHUNK_OVERLAP = 50  # ç‰‡æ®µé‡ç–Šå­—æ•¸
TOP_K = 5  # æª¢ç´¢è¿”å›çš„ç‰‡æ®µæ•¸é‡

# ============ åˆå§‹åŒ– ============

app = FastAPI(
    title="RAG æ‘˜è¦èˆ‡QA API",
    description="ä½¿ç”¨ RAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰æŠ€è¡“çš„æ™ºèƒ½å•ç­”ç³»çµ±ï¼Œæ”¯æ´å¤šæ–‡æª”ä¸Šå‚³å’Œå‘é‡æª¢ç´¢",
    version="2.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============ å…§å­˜å‘é‡è³‡æ–™åº« ============

class VectorStore:
    """ç°¡æ˜“å‘é‡è³‡æ–™åº«"""
    
    def __init__(self):
        self.documents: Dict[str, dict] = {}  # æ–‡æª”å…ƒæ•¸æ“š
        self.chunks: List[dict] = []  # æ‰€æœ‰ç‰‡æ®µ
        self.embeddings: List[List[float]] = []  # å°æ‡‰çš„å‘é‡
    
    def add_document(self, doc_id: str, title: str, content: str, chunks: List[str], embeddings: List[List[float]]):
        """æ·»åŠ æ–‡æª”"""
        self.documents[doc_id] = {
            "id": doc_id,
            "title": title,
            "content": content,
            "content_length": len(content),
            "chunks_count": len(chunks),
            "created_at": datetime.now().isoformat()
        }
        
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            self.chunks.append({
                "id": f"{doc_id}_{i}",
                "document_id": doc_id,
                "title": title,
                "content": chunk,
                "chunk_index": i
            })
            self.embeddings.append(embedding)
    
    def delete_document(self, doc_id: str):
        """åˆªé™¤æ–‡æª”"""
        if doc_id not in self.documents:
            return False
        
        # æ‰¾å‡ºè¦åˆªé™¤çš„ç‰‡æ®µç´¢å¼•
        indices_to_remove = [i for i, c in enumerate(self.chunks) if c["document_id"] == doc_id]
        
        # å¾å¾Œå¾€å‰åˆªé™¤
        for i in reversed(indices_to_remove):
            del self.chunks[i]
            del self.embeddings[i]
        
        del self.documents[doc_id]
        return True
    
    def search(self, query_embedding: List[float], top_k: int = 5) -> List[dict]:
        """å‘é‡ç›¸ä¼¼åº¦æœç´¢"""
        if not self.embeddings:
            return []
        
        # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
        scores = []
        for i, emb in enumerate(self.embeddings):
            score = self._cosine_similarity(query_embedding, emb)
            scores.append((i, score))
        
        # æ’åºä¸¦è¿”å› top_k
        scores.sort(key=lambda x: x[1], reverse=True)
        
        results = []
        for i, score in scores[:top_k]:
            chunk = self.chunks[i].copy()
            chunk["score"] = score
            results.append(chunk)
        
        return results
    
    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        dot_product = sum(x * y for x, y in zip(a, b))
        norm_a = math.sqrt(sum(x * x for x in a))
        norm_b = math.sqrt(sum(x * x for x in b))
        if norm_a == 0 or norm_b == 0:
            return 0.0
        return dot_product / (norm_a * norm_b)
    
    def clear(self):
        """æ¸…ç©ºæ‰€æœ‰æ•¸æ“š"""
        self.documents.clear()
        self.chunks.clear()
        self.embeddings.clear()
    
    def count_chunks(self) -> int:
        return len(self.chunks)
    
    def count_documents(self) -> int:
        return len(self.documents)


# åˆå§‹åŒ–å‘é‡å­˜å„²
vector_store = VectorStore()


# ============ å·¥å…·å‡½æ•¸ ============

def split_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:
    """å°‡æ–‡æœ¬åˆ†å‰²æˆå°å¡Š"""
    paragraphs = re.split(r'\n\s*\n', text)
    
    chunks = []
    current_chunk = ""
    
    for para in paragraphs:
        para = para.strip()
        if not para:
            continue
        
        if len(current_chunk) + len(para) <= chunk_size:
            current_chunk += ("\n\n" + para if current_chunk else para)
        else:
            if current_chunk:
                chunks.append(current_chunk)
            
            if len(para) > chunk_size:
                sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ.!?])\s*', para)
                current_chunk = ""
                for sent in sentences:
                    if len(current_chunk) + len(sent) <= chunk_size:
                        current_chunk += sent
                    else:
                        if current_chunk:
                            chunks.append(current_chunk)
                        current_chunk = sent
            else:
                current_chunk = para
    
    if current_chunk:
        chunks.append(current_chunk)
    
    # å¦‚æœæ²’æœ‰åˆ†å‰²æˆåŠŸï¼Œè‡³å°‘è¿”å›åŸæ–‡
    if not chunks and text.strip():
        chunks = [text.strip()]
    
    return chunks


async def get_embedding(text: str) -> List[float]:
    """ä½¿ç”¨ Ollama ç²å–æ–‡æœ¬åµŒå…¥å‘é‡"""
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{OLLAMA_BASE_URL}/api/embeddings",
                json={
                    "model": EMBEDDING_MODEL,
                    "prompt": text
                }
            )
            
            if response.status_code != 200:
                raise HTTPException(
                    status_code=500,
                    detail=f"åµŒå…¥ç”Ÿæˆå¤±æ•—: {response.text}"
                )
            
            result = response.json()
            return result.get("embedding", [])
            
    except httpx.ConnectError:
        raise HTTPException(
            status_code=503,
            detail=f"ç„¡æ³•é€£æ¥åˆ° Ollamaã€‚è«‹ç¢ºèªå·²å•Ÿå‹•ä¸¦ä¸‹è¼‰åµŒå…¥æ¨¡å‹: ollama pull {EMBEDDING_MODEL}"
        )


async def get_embeddings(texts: List[str]) -> List[List[float]]:
    """æ‰¹é‡ç²å–åµŒå…¥å‘é‡"""
    embeddings = []
    for text in texts:
        emb = await get_embedding(text)
        embeddings.append(emb)
    return embeddings


async def call_ollama(prompt: str, system_prompt: str = "") -> str:
    """èª¿ç”¨ Ollama LLM"""
    try:
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                f"{OLLAMA_BASE_URL}/api/generate",
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": prompt,
                    "system": system_prompt,
                    "stream": False
                }
            )
            
            if response.status_code != 200:
                raise HTTPException(status_code=500, detail=f"Ollama è«‹æ±‚å¤±æ•—: {response.text}")
            
            result = response.json()
            return result.get("response", "").strip()
            
    except httpx.ConnectError:
        raise HTTPException(
            status_code=503,
            detail="ç„¡æ³•é€£æ¥åˆ° Ollamaã€‚è«‹åŸ·è¡Œ 'ollama serve'"
        )
    except httpx.TimeoutException:
        raise HTTPException(status_code=504, detail="Ollama å›æ‡‰è¶…æ™‚")


# ============ è«‹æ±‚/å›æ‡‰æ¨¡å‹ ============

class DocumentUploadRequest(BaseModel):
    content: str = Field(..., description="æ–‡æª”å…§å®¹", min_length=10)
    title: str = Field(default="æœªå‘½åæ–‡æª”", description="æ–‡æª”æ¨™é¡Œ")


class RAGQueryRequest(BaseModel):
    question: str = Field(..., description="è¦å›ç­”çš„å•é¡Œ", min_length=3)
    top_k: int = Field(default=5, description="æª¢ç´¢ç‰‡æ®µæ•¸é‡", ge=1, le=20)
    language: str = Field(default="zh-TW", description="è¼¸å‡ºèªè¨€")


class SummaryRequest(BaseModel):
    document_id: str = Field(..., description="æ–‡æª” ID")
    max_length: int = Field(default=200, description="æ‘˜è¦æœ€å¤§é•·åº¦", ge=10, le=1000)
    language: str = Field(default="zh-TW", description="è¼¸å‡ºèªè¨€")


class DocumentResponse(BaseModel):
    document_id: str
    title: str
    content_length: int
    chunks_count: int
    created_at: str


class RAGQueryResponse(BaseModel):
    question: str
    answer: str
    sources: List[dict]
    confidence: str


# ============ API ç«¯é» ============

@app.get("/")
async def root():
    """API æ ¹ç«¯é»"""
    return {
        "message": "ğŸš€ æ­¡è¿ä½¿ç”¨ RAG æ‘˜è¦èˆ‡QA API",
        "version": "2.0.0",
        "features": [
            "ğŸ“š RAG æª¢ç´¢å¢å¼·ç”Ÿæˆ",
            "ğŸ” å¤šæ–‡æª”çŸ¥è­˜åº«",
            "ğŸ¯ å‘é‡èªç¾©æœç´¢",
            "ğŸ“ æ™ºèƒ½æ‘˜è¦ç”Ÿæˆ"
        ],
        "stats": {
            "documents": vector_store.count_documents(),
            "total_chunks": vector_store.count_chunks(),
            "embedding_model": EMBEDDING_MODEL,
            "llm_model": OLLAMA_MODEL
        },
        "endpoints": {
            "ä¸Šå‚³æ–‡æª”": "POST /api/documents",
            "åˆ—å‡ºæ–‡æª”": "GET /api/documents",
            "RAG å•ç­”": "POST /api/rag/query",
            "èªç¾©æœç´¢": "POST /api/rag/search",
            "ç”Ÿæˆæ‘˜è¦": "POST /api/summary",
            "API æ–‡æª”": "/docs"
        }
    }


@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥"""
    ollama_status = "unknown"
    embedding_status = "unknown"
    
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            # æª¢æŸ¥ Ollama
            response = await client.get(f"{OLLAMA_BASE_URL}/api/tags")
            if response.status_code == 200:
                ollama_status = "healthy"
                models = response.json().get("models", [])
                model_names = [m.get("name", "") for m in models]
                
                # æª¢æŸ¥åµŒå…¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
                if any(EMBEDDING_MODEL in name for name in model_names):
                    embedding_status = "ready"
                else:
                    embedding_status = f"missing - è«‹åŸ·è¡Œ: ollama pull {EMBEDDING_MODEL}"
            else:
                ollama_status = "error"
    except Exception:
        ollama_status = "unreachable"
    
    return {
        "status": "healthy",
        "ollama_status": ollama_status,
        "embedding_status": embedding_status,
        "llm_model": OLLAMA_MODEL,
        "embedding_model": EMBEDDING_MODEL,
        "documents_count": vector_store.count_documents(),
        "chunks_count": vector_store.count_chunks()
    }


# ============ æ–‡æª”ç®¡ç† ============

@app.post("/api/documents", response_model=DocumentResponse)
async def upload_document(request: DocumentUploadRequest):
    """
    ğŸ“¤ ä¸Šå‚³æ–‡æª”åˆ°çŸ¥è­˜åº«
    
    æ–‡æª”æœƒè¢«è‡ªå‹•åˆ†å‰²ä¸¦å»ºç«‹å‘é‡ç´¢å¼•ï¼Œç”¨æ–¼å¾ŒçºŒçš„ RAG å•ç­”ã€‚
    """
    document_id = str(uuid.uuid4())[:8]
    
    # åˆ†å‰²æ–‡æª”
    chunks = split_text(request.content)
    
    if not chunks:
        raise HTTPException(status_code=400, detail="æ–‡æª”å…§å®¹å¤ªçŸ­")
    
    # ç”ŸæˆåµŒå…¥å‘é‡
    print(f"æ­£åœ¨ç‚º {len(chunks)} å€‹ç‰‡æ®µç”ŸæˆåµŒå…¥å‘é‡...")
    embeddings = await get_embeddings(chunks)
    
    # å­˜å…¥å‘é‡è³‡æ–™åº«
    vector_store.add_document(
        doc_id=document_id,
        title=request.title,
        content=request.content,
        chunks=chunks,
        embeddings=embeddings
    )
    
    doc = vector_store.documents[document_id]
    
    return DocumentResponse(
        document_id=document_id,
        title=request.title,
        content_length=doc["content_length"],
        chunks_count=doc["chunks_count"],
        created_at=doc["created_at"]
    )


@app.get("/api/documents")
async def list_documents():
    """ğŸ“‹ åˆ—å‡ºæ‰€æœ‰æ–‡æª”"""
    documents = []
    for doc_id, doc in vector_store.documents.items():
        documents.append({
            "document_id": doc_id,
            "title": doc["title"],
            "content_length": doc["content_length"],
            "chunks_count": doc["chunks_count"],
            "created_at": doc["created_at"],
            "preview": doc["content"][:100] + "..." if len(doc["content"]) > 100 else doc["content"]
        })
    
    return {
        "total": len(documents),
        "total_chunks": vector_store.count_chunks(),
        "documents": documents
    }


@app.get("/api/documents/{document_id}")
async def get_document(document_id: str):
    """ğŸ“„ ç²å–ç‰¹å®šæ–‡æª”"""
    if document_id not in vector_store.documents:
        raise HTTPException(status_code=404, detail=f"æ‰¾ä¸åˆ°æ–‡æª” ID: {document_id}")
    
    return vector_store.documents[document_id]


@app.delete("/api/documents/{document_id}")
async def delete_document(document_id: str):
    """ğŸ—‘ï¸ åˆªé™¤æ–‡æª”"""
    if not vector_store.delete_document(document_id):
        raise HTTPException(status_code=404, detail=f"æ‰¾ä¸åˆ°æ–‡æª” ID: {document_id}")
    
    return {"message": f"å·²æˆåŠŸåˆªé™¤æ–‡æª” {document_id}"}


@app.delete("/api/documents")
async def clear_all_documents():
    """ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ–‡æª”"""
    vector_store.clear()
    return {"message": "å·²æ¸…ç©ºæ‰€æœ‰æ–‡æª”"}


# ============ RAG åŠŸèƒ½ ============

@app.post("/api/rag/search")
async def semantic_search(request: RAGQueryRequest):
    """
    ğŸ” èªç¾©æœç´¢
    
    åœ¨çŸ¥è­˜åº«ä¸­æœç´¢èˆ‡å•é¡Œæœ€ç›¸é—œçš„æ–‡æª”ç‰‡æ®µã€‚
    """
    if vector_store.count_chunks() == 0:
        raise HTTPException(status_code=400, detail="çŸ¥è­˜åº«ç‚ºç©ºï¼Œè«‹å…ˆä¸Šå‚³æ–‡æª”")
    
    # ç²å–å•é¡Œçš„åµŒå…¥å‘é‡
    query_embedding = await get_embedding(request.question)
    
    # æœç´¢
    results = vector_store.search(query_embedding, request.top_k)
    
    return {
        "question": request.question,
        "results_count": len(results),
        "sources": [
            {
                "document_title": r["title"],
                "content": r["content"],
                "relevance_score": round(r["score"], 3)
            }
            for r in results
        ]
    }


@app.post("/api/rag/query", response_model=RAGQueryResponse)
async def rag_query(request: RAGQueryRequest):
    """
    ğŸ¤– RAG å•ç­”
    
    ä½¿ç”¨æª¢ç´¢å¢å¼·ç”Ÿæˆï¼ˆRAGï¼‰æŠ€è¡“å›ç­”å•é¡Œï¼š
    1. åœ¨çŸ¥è­˜åº«ä¸­æœç´¢ç›¸é—œç‰‡æ®µ
    2. å°‡ç‰‡æ®µä½œç‚ºä¸Šä¸‹æ–‡å‚³çµ¦ LLM
    3. LLM æ ¹æ“šä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
    """
    if vector_store.count_chunks() == 0:
        raise HTTPException(status_code=400, detail="çŸ¥è­˜åº«ç‚ºç©ºï¼Œè«‹å…ˆä¸Šå‚³æ–‡æª”")
    
    # ç²å–å•é¡ŒåµŒå…¥
    query_embedding = await get_embedding(request.question)
    
    # æœç´¢ç›¸é—œç‰‡æ®µ
    results = vector_store.search(query_embedding, request.top_k)
    
    # çµ„åˆä¸Šä¸‹æ–‡
    context_parts = []
    sources = []
    for r in results:
        context_parts.append(f"[ä¾†æº: {r['title']}]\n{r['content']}")
        sources.append({
            "document_title": r["title"],
            "content": r["content"][:200] + "..." if len(r["content"]) > 200 else r["content"],
            "relevance_score": round(r["score"], 3)
        })
    
    context = "\n\n---\n\n".join(context_parts)
    
    # èªè¨€è¨­å®š
    language_map = {
        "zh-TW": "ç¹é«”ä¸­æ–‡",
        "zh-CN": "ç®€ä½“ä¸­æ–‡",
        "en": "English"
    }
    target_lang = language_map.get(request.language, "ç¹é«”ä¸­æ–‡")
    
    # æ§‹å»ºæç¤ºè©
    system_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å•ç­”åŠ©æ‰‹ã€‚è«‹æ ¹æ“šæä¾›çš„åƒè€ƒè³‡æ–™å›ç­”å•é¡Œã€‚
è¦å‰‡ï¼š
1. åªæ ¹æ“šåƒè€ƒè³‡æ–™ä¸­çš„ä¿¡æ¯å›ç­”
2. å¦‚æœè³‡æ–™ä¸­æ²’æœ‰ç›¸é—œä¿¡æ¯ï¼Œè«‹æ˜ç¢ºèªªæ˜
3. å›ç­”è¦æº–ç¢ºã€æœ‰æ¢ç†
4. é©ç•¶å¼•ç”¨ä¾†æº"""
    
    prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹åƒè€ƒè³‡æ–™å›ç­”å•é¡Œã€‚

## åƒè€ƒè³‡æ–™
{context}

## å•é¡Œ
{request.question}

è«‹ç”¨{target_lang}å›ç­”ã€‚"""
    
    # èª¿ç”¨ LLM
    answer = await call_ollama(prompt, system_prompt)
    
    # è¨ˆç®—ä¿¡å¿ƒç¨‹åº¦
    if results:
        avg_score = sum(r["score"] for r in results) / len(results)
        if avg_score > 0.7:
            confidence = "high"
        elif avg_score > 0.5:
            confidence = "medium"
        else:
            confidence = "low"
    else:
        confidence = "low"
    
    return RAGQueryResponse(
        question=request.question,
        answer=answer,
        sources=sources,
        confidence=confidence
    )


# ============ æ‘˜è¦åŠŸèƒ½ ============

@app.post("/api/summary")
async def create_summary(request: SummaryRequest):
    """
    ğŸ“ ç”Ÿæˆæ–‡æª”æ‘˜è¦
    """
    if request.document_id not in vector_store.documents:
        raise HTTPException(status_code=404, detail=f"æ‰¾ä¸åˆ°æ–‡æª” ID: {request.document_id}")
    
    doc = vector_store.documents[request.document_id]
    text = doc["content"]
    
    language_map = {
        "zh-TW": "ç¹é«”ä¸­æ–‡",
        "zh-CN": "ç®€ä½“ä¸­æ–‡",
        "en": "English"
    }
    target_lang = language_map.get(request.language, "ç¹é«”ä¸­æ–‡")
    
    system_prompt = "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡æœ¬æ‘˜è¦åŠ©æ‰‹ã€‚"
    
    prompt = f"""è«‹ç‚ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆæ‘˜è¦ã€‚

è¦æ±‚ï¼š
1. æ‘˜è¦ä¸è¶…é {request.max_length} å­—
2. ä½¿ç”¨ {target_lang}
3. ä¿ç•™é—œéµä¿¡æ¯

æ–‡æœ¬ï¼š
{text}

è«‹ç›´æ¥è¼¸å‡ºæ‘˜è¦ã€‚"""
    
    summary = await call_ollama(prompt, system_prompt)
    
    return {
        "document_id": request.document_id,
        "title": doc["title"],
        "original_length": len(text),
        "summary": summary,
        "summary_length": len(summary)
    }


# ============ å•Ÿå‹• ============

if __name__ == "__main__":
    import uvicorn
    
    print("=" * 60)
    print("RAG Summary & QA API v2.0")
    print("=" * 60)
    print(f"LLM Model: {OLLAMA_MODEL}")
    print(f"Embedding Model: {EMBEDDING_MODEL}")
    print(f"Chunk Size: {CHUNK_SIZE}")
    print(f"Top K: {TOP_K}")
    print("=" * 60)
    print("Please ensure:")
    print(f"  1. Ollama is running: ollama serve")
    print(f"  2. LLM downloaded: ollama pull {OLLAMA_MODEL}")
    print(f"  3. Embedding model: ollama pull {EMBEDDING_MODEL}")
    print("=" * 60)
    
    uvicorn.run(app, host="0.0.0.0", port=8000)
